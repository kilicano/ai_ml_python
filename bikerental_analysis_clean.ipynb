{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Date: 2025-12-25\n",
    "# Bike Rental Forecasting – Clean, single-cell notebook\n",
    "# -----------------------------------------------------\n",
    "# This notebook is intentionally a single code cell to make the workflow easy to follow end-to-end.\n",
    "# It prints intermediate results at each decision point, so it reads like an executable report.\n",
    "#\n",
    "# Adjust the paths below to your local files\n",
    "HOUR_CSV_PATH = r'hour.csv'   # <-- change if needed\n",
    "DAY_CSV_PATH  = r'day.csv'    # optional; only used for consistency cross-checks\n",
    "\n",
    "ANALYSE = True               # toggles extra plots & model comparison output\n",
    "FORECAST_HOURS = 24          # how many future hours to forecast beyond the last observed timestamp\n",
    "K_MISSING_THRESHOLD = 2      # <=K => human_error; >K => extraordinary_event (with manual overrides)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# -----------------------------\n",
    "# Pretty printing helpers\n",
    "# -----------------------------\n",
    "def section(title: str):\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(title)\n",
    "    print(\"=\"*90)\n",
    "\n",
    "def show_df(df: pd.DataFrame, n=8, name=None):\n",
    "    if name:\n",
    "        print(f\"\\n{name} (showing first {n} rows):\")\n",
    "    display(df.head(n)) if \"display\" in globals() else print(df.head(n))\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load data\n",
    "# -----------------------------\n",
    "section(\"1) Load data\")\n",
    "\n",
    "hour_path = HOUR_CSV_PATH\n",
    "if not Path(hour_path).exists():\n",
    "    raise FileNotFoundError(f\"Cannot find {HOUR_CSV_PATH}. Set HOUR_CSV_PATH to your local hour.csv path.\")\n",
    "\n",
    "df_hourly = pd.read_csv(hour_path)\n",
    "\n",
    "print(\"Hourly shape:\", df_hourly.shape)\n",
    "print(\"Hourly columns:\", list(df_hourly.columns))\n",
    "\n",
    "# Optional daily file for consistency checks\n",
    "df_daily = None\n",
    "day_path = DAY_CSV_PATH\n",
    "if Path(day_path).exists():\n",
    "    df_daily = pd.read_csv(day_path)\n",
    "    print(\"Daily shape:\", df_daily.shape)\n",
    "else:\n",
    "    print(\"Daily file not found (that's ok). DAY_CSV_PATH can be left as-is if you only work on hourly.\")\n",
    "\n",
    "# Ensure types & sorting\n",
    "df_hourly[\"dteday\"] = pd.to_datetime(df_hourly[\"dteday\"])\n",
    "df_hourly[\"hr\"] = pd.to_numeric(df_hourly[\"hr\"], errors=\"coerce\").astype(int)\n",
    "df_hourly = df_hourly.sort_values([\"dteday\", \"hr\"]).reset_index(drop=True)\n",
    "\n",
    "if df_daily is not None:\n",
    "    df_daily[\"dteday\"] = pd.to_datetime(df_daily[\"dteday\"])\n",
    "    df_daily = df_daily.sort_values(\"dteday\").reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Core sanity checks\n",
    "# -----------------------------\n",
    "section(\"2) Core sanity checks (identity + duplicates + missing days)\")\n",
    "\n",
    "# cnt identity check\n",
    "hourly_identity_ok = (df_hourly[\"casual\"] + df_hourly[\"registered\"] == df_hourly[\"cnt\"]).all()\n",
    "print(\"Hourly cnt = casual + registered:\", hourly_identity_ok)\n",
    "\n",
    "if df_daily is not None and set([\"casual\",\"registered\",\"cnt\"]).issubset(df_daily.columns):\n",
    "    daily_identity_ok = (df_daily[\"casual\"] + df_daily[\"registered\"] == df_daily[\"cnt\"]).all()\n",
    "    print(\"Daily cnt = casual + registered:\", daily_identity_ok)\n",
    "\n",
    "# duplicates check\n",
    "dup = df_hourly.duplicated(subset=[\"dteday\",\"hr\"]).sum()\n",
    "print(\"Duplicate (dteday, hr):\", dup)\n",
    "\n",
    "# missing entire days check\n",
    "full_days = pd.date_range(df_hourly[\"dteday\"].min(), df_hourly[\"dteday\"].max(), freq=\"D\")\n",
    "present_days = pd.Index(df_hourly[\"dteday\"].unique())\n",
    "missing_days = full_days.difference(present_days)\n",
    "\n",
    "print(f\"Missing entire days: {len(missing_days)}\")\n",
    "if len(missing_days):\n",
    "    print(missing_days)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Missing hours per day (gaps) + classification\n",
    "# -----------------------------\n",
    "section(\"3) Missing hours per day and gap classification\")\n",
    "\n",
    "missing_by_day = (\n",
    "    df_hourly.groupby(\"dteday\")[\"hr\"]\n",
    "    .apply(lambda x: sorted(set(range(24)) - set(x)))\n",
    ")\n",
    "missing_by_day = missing_by_day[missing_by_day.apply(len) > 0]\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    \"missing_hours\": missing_by_day,\n",
    "    \"n_missing_hours\": missing_by_day.apply(len)\n",
    "}).sort_values(\"n_missing_hours\", ascending=False)\n",
    "\n",
    "print(\"Days with missing hours:\", len(missing_summary))\n",
    "print(\"\\nMissing-hour count distribution:\")\n",
    "print(missing_summary[\"n_missing_hours\"].value_counts().sort_index())\n",
    "\n",
    "if len(missing_summary):\n",
    "    print(\"\\nTop days by missing hours:\")\n",
    "    print(missing_summary.head(10))\n",
    "\n",
    "# Manual event mapping (based on your investigation)\n",
    "EVENTS = {\n",
    "    \"2011-08-27\": \"Hurricane Irene (evening shutdown)\",\n",
    "    \"2011-01-26\": \"Heavy snowfall\",\n",
    "    \"2011-01-27\": \"Heavy snowfall\",\n",
    "    \"2011-01-18\": \"Protests (Hu Jintao visit)\",\n",
    "    \"2012-10-29\": \"Hurricane Sandy\",\n",
    "    \"2012-10-30\": \"Hurricane Sandy\",\n",
    "}\n",
    "HUMAN_OVERRIDE = [\"2011-02-22\"]  # treat as human error even if many hours missing\n",
    "\n",
    "event_days = pd.to_datetime(list(EVENTS.keys()))\n",
    "human_override_days = pd.to_datetime(HUMAN_OVERRIDE)\n",
    "\n",
    "small_gap_days = missing_summary[missing_summary[\"n_missing_hours\"] <= K_MISSING_THRESHOLD].index\n",
    "large_gap_days = missing_summary[missing_summary[\"n_missing_hours\"] > K_MISSING_THRESHOLD].index\n",
    "\n",
    "# Apply overrides\n",
    "large_gap_days = large_gap_days.union(event_days).difference(human_override_days)\n",
    "small_gap_days = small_gap_days.union(human_override_days).difference(event_days)\n",
    "\n",
    "print(f\"\\nHuman error days (<= {K_MISSING_THRESHOLD} missing hours, with overrides): {len(small_gap_days)}\")\n",
    "print(f\"Extraordinary event days (> {K_MISSING_THRESHOLD} missing hours, with overrides): {len(large_gap_days)}\")\n",
    "\n",
    "event_table = pd.DataFrame({\n",
    "    \"dteday\": event_days,\n",
    "    \"event\": [EVENTS[d.strftime(\"%Y-%m-%d\")] for d in event_days]\n",
    "}).sort_values(\"dteday\")\n",
    "\n",
    "print(\"\\nExtraordinary event mapping:\")\n",
    "print(event_table.to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Repair to full 24h/day grid + imputation rules\n",
    "# -----------------------------\n",
    "section(\"4) Repair hourly grid and fill missing values based on gap type\")\n",
    "\n",
    "TARGET_COLS = [\"cnt\",\"casual\",\"registered\"]\n",
    "CAL_COLS = [\"season\",\"yr\",\"mnth\"]\n",
    "DAY_FLAG_COLS = [\"holiday\",\"weekday\",\"workingday\"]\n",
    "WEATHER_COLS = [\"weathersit\",\"temp\",\"atemp\",\"hum\",\"windspeed\"]\n",
    "\n",
    "df = df_hourly.copy()\n",
    "\n",
    "# Ensure numeric for fillable columns\n",
    "for c in TARGET_COLS + CAL_COLS + DAY_FLAG_COLS + WEATHER_COLS:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Build full day x hour grid\n",
    "all_days = pd.Index(df[\"dteday\"].unique()).sort_values()\n",
    "full_index = pd.MultiIndex.from_product([all_days, range(24)], names=[\"dteday\",\"hr\"])\n",
    "\n",
    "df_full = (\n",
    "    df.set_index([\"dteday\",\"hr\"])\n",
    "      .reindex(full_index)\n",
    "      .sort_index()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Timestamp convenience column\n",
    "df_full[\"timestamp\"] = df_full[\"dteday\"] + pd.to_timedelta(df_full[\"hr\"], unit=\"h\")\n",
    "\n",
    "# Assign gap_type for each day\n",
    "df_full[\"gap_type\"] = \"none\"\n",
    "df_full.loc[df_full[\"dteday\"].isin(small_gap_days), \"gap_type\"] = \"human_error\"\n",
    "df_full.loc[df_full[\"dteday\"].isin(large_gap_days), \"gap_type\"] = \"extraordinary_event\"\n",
    "\n",
    "row_missing = df_full[\"cnt\"].isna()\n",
    "mask_human_missing = (df_full[\"gap_type\"] == \"human_error\") & row_missing\n",
    "mask_event_missing = (df_full[\"gap_type\"] == \"extraordinary_event\") & row_missing\n",
    "\n",
    "# 4.1 Human error: interpolate within day for targets + weather\n",
    "for col in TARGET_COLS + WEATHER_COLS:\n",
    "    df_full.loc[df_full[\"gap_type\"] == \"human_error\", col] = (\n",
    "        df_full.loc[df_full[\"gap_type\"] == \"human_error\"]\n",
    "        .groupby(\"dteday\")[col]\n",
    "        .transform(lambda s: s.interpolate(limit_direction=\"both\"))\n",
    "    )\n",
    "\n",
    "# 4.2 Extraordinary events:\n",
    "# - targets for missing rows only -> 0\n",
    "df_full.loc[mask_event_missing, TARGET_COLS] = 0.0\n",
    "\n",
    "# - calendar fields should be constant within day -> forward fill within day\n",
    "for col in CAL_COLS:\n",
    "    df_full[col] = df_full.groupby(\"dteday\")[col].ffill()\n",
    "\n",
    "# - recompute weekday + workingday, keep holiday mode if present\n",
    "def fill_day_flags(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = group.name\n",
    "    weekday = d.weekday()  # 0=Mon … 6=Sun\n",
    "    group[\"weekday\"] = weekday\n",
    "\n",
    "    # if holiday exists and has any value, use mode; else assume 0\n",
    "    if group[\"holiday\"].notna().any():\n",
    "        holiday_val = int(group[\"holiday\"].mode().iloc[0])\n",
    "    else:\n",
    "        holiday_val = 0\n",
    "    group[\"holiday\"] = holiday_val\n",
    "    group[\"workingday\"] = int((weekday < 5) and (holiday_val == 0))\n",
    "    return group\n",
    "\n",
    "df_full = df_full.groupby(\"dteday\", group_keys=False).apply(fill_day_flags)\n",
    "\n",
    "# - weather for extraordinary-event missing rows: mean of nearest valid neighbors in time\n",
    "df_full = df_full.sort_values([\"dteday\",\"hr\"]).reset_index(drop=True)\n",
    "for col in WEATHER_COLS:\n",
    "    prev_val = df_full[col].ffill()\n",
    "    next_val = df_full[col].bfill()\n",
    "    df_full.loc[mask_event_missing, col] = (prev_val[mask_event_missing] + next_val[mask_event_missing]) / 2\n",
    "\n",
    "# Snap weathersit back to {1,2,3,4} (avoid 3.5 etc.)\n",
    "if \"weathersit\" in df_full.columns:\n",
    "    df_full[\"weathersit\"] = (\n",
    "        df_full[\"weathersit\"]\n",
    "        .round()\n",
    "        .clip(lower=1, upper=4)\n",
    "        .astype(\"Int64\")\n",
    "    )\n",
    "\n",
    "# Enforce cnt identity (after fills)\n",
    "identity_ok = (df_full[\"casual\"] + df_full[\"registered\"] == df_full[\"cnt\"])\n",
    "print(\"Identity cnt = casual + registered holds:\", identity_ok.all())\n",
    "\n",
    "# Remaining NaNs\n",
    "nan_counts = df_full[TARGET_COLS + CAL_COLS + DAY_FLAG_COLS + WEATHER_COLS].isna().sum().sort_values(ascending=False)\n",
    "print(\"\\nRemaining NaNs (top):\")\n",
    "print(nan_counts.head(15))\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Optional: consistency with daily file (hourly aggregated vs daily)\n",
    "# -----------------------------\n",
    "section(\"5) Optional cross-check: hourly aggregation matches daily file (if available)\")\n",
    "\n",
    "if df_daily is not None:\n",
    "    hourly_daily = (\n",
    "        df_full.groupby(\"dteday\")[[\"casual\",\"registered\",\"cnt\"]]\n",
    "              .sum()\n",
    "              .reset_index()\n",
    "    )\n",
    "    cmp = hourly_daily.merge(df_daily[[\"dteday\",\"casual\",\"registered\",\"cnt\"]],\n",
    "                             on=\"dteday\", suffixes=(\"_hourly\",\"_daily\"))\n",
    "    for col in [\"casual\",\"registered\",\"cnt\"]:\n",
    "        cmp[f\"{col}_equal\"] = cmp[f\"{col}_hourly\"] == cmp[f\"{col}_daily\"]\n",
    "    print(\"All equal:\")\n",
    "    print(cmp[[c for c in cmp.columns if c.endswith(\"_equal\")]].all())\n",
    "else:\n",
    "    print(\"Skipped (daily file not provided).\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) EDA: key plots + numeric summaries used for decisions\n",
    "# -----------------------------\n",
    "section(\"6) EDA snapshots (seasonality, weekday patterns, hourly profile, weather correlations)\")\n",
    "\n",
    "# Correlations with cnt (hourly)\n",
    "corr = df_full[[\"cnt\",\"temp\",\"atemp\",\"hum\",\"windspeed\"]].corr(numeric_only=True)[\"cnt\"].sort_values(ascending=False)\n",
    "print(\"Correlation with cnt:\")\n",
    "print(corr)\n",
    "\n",
    "# workingday/holiday group means\n",
    "grp = df_full.groupby([\"workingday\",\"holiday\"])[[\"cnt\",\"casual\",\"registered\"]].mean()\n",
    "print(\"\\nMean rentals by workingday/holiday:\")\n",
    "print(grp)\n",
    "\n",
    "if ANALYSE:\n",
    "    # Seasonal lines on daily totals (from hourly)\n",
    "    daily_series = df_full.groupby(\"dteday\")[[\"casual\",\"registered\"]].sum().reset_index()\n",
    "    # Attach season from first row of each day\n",
    "    day_season = df_full.groupby(\"dteday\")[\"season\"].first().reset_index()\n",
    "    daily_series = daily_series.merge(day_season, on=\"dteday\", how=\"left\").sort_values(\"dteday\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    ax.plot(daily_series[\"dteday\"], daily_series[\"casual\"], label=\"casual\")\n",
    "    ax.plot(daily_series[\"dteday\"], daily_series[\"registered\"], label=\"registered\")\n",
    "    season_change = daily_series[\"season\"].ne(daily_series[\"season\"].shift())\n",
    "    for x in daily_series.loc[season_change, \"dteday\"]:\n",
    "        ax.axvline(x=x, linestyle=\"--\", alpha=0.35)\n",
    "    ax.set_xlabel(\"dteday\")\n",
    "    ax.set_ylabel(\"Daily count\")\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=4))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b\"))\n",
    "    fig.autofmt_xdate()\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Daily totals (casual vs registered) with season change markers\")\n",
    "    plt.show()\n",
    "\n",
    "    # Weekday profile (hourly mean)\n",
    "    by_weekday = df_full.groupby(\"weekday\")[[\"casual\",\"registered\"]].mean().reset_index()\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.plot(by_weekday[\"weekday\"], by_weekday[\"casual\"], label=\"casual\")\n",
    "    plt.plot(by_weekday[\"weekday\"], by_weekday[\"registered\"], label=\"registered\")\n",
    "    plt.xlabel(\"weekday (0=Mon)\")\n",
    "    plt.ylabel(\"Avg hourly count\")\n",
    "    plt.title(\"Average hourly rentals by weekday\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Hour-of-day profile\n",
    "    by_hour = df_full.groupby(\"hr\")[[\"casual\",\"registered\"]].mean().reset_index()\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.plot(by_hour[\"hr\"], by_hour[\"casual\"], label=\"casual\")\n",
    "    plt.plot(by_hour[\"hr\"], by_hour[\"registered\"], label=\"registered\")\n",
    "    plt.xlabel(\"hour of day\")\n",
    "    plt.ylabel(\"Avg hourly count\")\n",
    "    plt.xticks(range(0,24))\n",
    "    plt.title(\"Average rentals by hour of day (cnt = casual + registered)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Feature engineering (final set used for modeling)\n",
    "# -----------------------------\n",
    "section(\"7) Feature engineering (final feature set used for modeling)\")\n",
    "\n",
    "TARGET = \"cnt\"\n",
    "df_feat = df_full.copy()\n",
    "df_feat[\"dteday\"] = pd.to_datetime(df_feat[\"dteday\"])\n",
    "\n",
    "# Lag features\n",
    "df_feat[\"lag_24\"] = df_feat[TARGET].shift(24)\n",
    "df_feat[\"lag_168\"] = df_feat[TARGET].shift(168)\n",
    "\n",
    "# Season dummies\n",
    "season_map = {1:\"spring\", 2:\"summer\", 3:\"fall\", 4:\"winter\"}\n",
    "df_feat[\"season_name\"] = df_feat[\"season\"].map(season_map)\n",
    "season_dummies = pd.get_dummies(df_feat[\"season_name\"], prefix=\"season\")\n",
    "\n",
    "# Hour bucket dummies\n",
    "def hr_to_period(hr):\n",
    "    if 0 <= hr <= 5: return \"night\"\n",
    "    if 6 <= hr <= 11: return \"morning\"\n",
    "    if 12 <= hr <= 17: return \"afternoon\"\n",
    "    return \"evening\"\n",
    "\n",
    "df_feat[\"hr_period\"] = df_feat[\"hr\"].apply(hr_to_period)\n",
    "hr_dummies = pd.get_dummies(df_feat[\"hr_period\"], prefix=\"hr\")\n",
    "\n",
    "# Regime features\n",
    "df_feat[\"wd_morning\"] = ((df_feat[\"workingday\"] == 1) & (df_feat[\"hr_period\"] == \"morning\")).astype(int)\n",
    "df_feat[\"wd_evening\"] = ((df_feat[\"workingday\"] == 1) & (df_feat[\"hr_period\"] == \"evening\")).astype(int)\n",
    "\n",
    "BEHAVIOR_FEATURES = [\"wd_morning\",\"wd_evening\",\"holiday\",\"workingday\"]\n",
    "NUM_FEATURES = [\"lag_24\",\"lag_168\"]\n",
    "\n",
    "# Weather buckets (interpretable)\n",
    "df_feat[\"atemp_bucket\"] = pd.qcut(df_feat[\"atemp\"], q=3, labels=[\"cold\",\"medium\",\"hot\"])\n",
    "df_feat[\"hum_bucket\"] = pd.qcut(df_feat[\"hum\"], q=3, labels=[\"not_humid\",\"medium\",\"humid\"])\n",
    "df_feat[\"windspeed_bucket\"] = pd.qcut(df_feat[\"windspeed\"], q=3, labels=[\"not_windy\",\"medium\",\"windy\"])\n",
    "\n",
    "atemp_dummies = pd.get_dummies(df_feat[\"atemp_bucket\"], prefix=\"atemp\")\n",
    "hum_dummies = pd.get_dummies(df_feat[\"hum_bucket\"], prefix=\"hum\")\n",
    "wind_dummies = pd.get_dummies(df_feat[\"windspeed_bucket\"], prefix=\"wind\")\n",
    "\n",
    "# Weather situation dummies\n",
    "weathersit_dummies = pd.get_dummies(df_feat[\"weathersit\"].astype(\"Int64\"), prefix=\"weathersit\")\n",
    "\n",
    "# Final X/y\n",
    "X = pd.concat(\n",
    "    [\n",
    "        df_feat[NUM_FEATURES + BEHAVIOR_FEATURES],\n",
    "        season_dummies,\n",
    "        hr_dummies,\n",
    "        atemp_dummies,\n",
    "        hum_dummies,\n",
    "        wind_dummies,\n",
    "        weathersit_dummies,\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "y = df_feat[TARGET]\n",
    "\n",
    "data = pd.concat([X, y], axis=1).dropna()\n",
    "X = data.drop(columns=[TARGET])\n",
    "y = data[TARGET]\n",
    "\n",
    "# Convert booleans to ints\n",
    "bool_cols = X.select_dtypes(include=\"bool\").columns\n",
    "X[bool_cols] = X[bool_cols].astype(int)\n",
    "\n",
    "# Drop one baseline per dummy block to avoid dummy trap\n",
    "REF_COLS = [\"hr_night\",\"season_winter\",\"atemp_medium\",\"hum_medium\",\"wind_medium\",\"weathersit_1\"]\n",
    "X = X.drop(columns=[c for c in REF_COLS if c in X.columns], errors=\"ignore\")\n",
    "\n",
    "print(\"Rows after feature prep:\", len(X))\n",
    "print(\"Features:\", len(X.columns))\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Time-based train/test split + model comparison\n",
    "# -----------------------------\n",
    "section(\"8) Model comparison (time split, MAE)\")\n",
    "\n",
    "# Time split: last 20% as test\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(\"Train rows:\", len(X_train), \"Test rows:\", len(X_test))\n",
    "\n",
    "# Baseline: seasonal naive using lag_24\n",
    "# (Needs alignment: y_test corresponds to rows in X_test)\n",
    "naive_pred = X_test[\"lag_24\"].values\n",
    "mae_naive = mean_absolute_error(y_test, naive_pred)\n",
    "\n",
    "# Ridge\n",
    "ridge = Ridge(alpha=1.0, random_state=0)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "mae_ridge = mean_absolute_error(y_test, ridge_pred)\n",
    "\n",
    "# Gradient boosting (final choice)\n",
    "gbr = HistGradientBoostingRegressor(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    max_iter=400,\n",
    "    random_state=0\n",
    ")\n",
    "gbr.fit(X_train, y_train)\n",
    "gbr_pred = gbr.predict(X_test)\n",
    "mae_gbr = mean_absolute_error(y_test, gbr_pred)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Seasonal Naive (lag_24)\", \"Ridge Regression\", \"HistGradientBoostingRegressor\"],\n",
    "    \"MAE\": [mae_naive, mae_ridge, mae_gbr]\n",
    "}).sort_values(\"MAE\")\n",
    "\n",
    "print(\"\\nModel comparison (lower MAE is better):\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "if ANALYSE:\n",
    "    # Plot a week of predictions vs actual\n",
    "    n = min(24*7, len(y_test))\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(y_test.iloc[:n].values, label=\"actual\")\n",
    "    plt.plot(gbr_pred[:n], label=\"gbr_pred\")\n",
    "    plt.plot(ridge_pred[:n], label=\"ridge_pred\", alpha=0.7)\n",
    "    plt.title(\"Predicted vs actual (first ~week of test set)\")\n",
    "    plt.xlabel(\"Test hour index\")\n",
    "    plt.ylabel(\"cnt\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 9) Feature importance sanity check (GBR vs Ridge)\n",
    "# -----------------------------\n",
    "section(\"9) Feature importance comparison (does GBR align with Ridge signals?)\")\n",
    "\n",
    "# Ridge \"importance\": absolute coefficient\n",
    "ridge_imp = pd.Series(np.abs(ridge.coef_), index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# GBR: permutation importance on a subsample for speed\n",
    "rng = np.random.default_rng(0)\n",
    "sub_idx = rng.choice(np.arange(len(X_test)), size=min(2000, len(X_test)), replace=False)\n",
    "perm = permutation_importance(gbr, X_test.iloc[sub_idx], y_test.iloc[sub_idx],\n",
    "                              n_repeats=5, random_state=0, scoring=\"neg_mean_absolute_error\")\n",
    "\n",
    "gbr_imp = pd.Series(perm.importances_mean, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "top = 15\n",
    "imp_compare = pd.DataFrame({\n",
    "    \"ridge_importance\": ridge_imp.head(top),\n",
    "    \"gbr_importance\": gbr_imp.head(top),\n",
    "}).fillna(0).sort_values(by=\"gbr_importance\",ascending=False)\n",
    "\n",
    "print(f\"Top {top} features (Ridge abs(coef) and GBR permutation importance):\")\n",
    "print(imp_compare)\n",
    "\n",
    "# -----------------------------\n",
    "# 10) Simple forward forecast example (next day / next horizon)\n",
    "# -----------------------------\n",
    "section(\"10) Forward forecast example (recursive, next unseen hours)\")\n",
    "\n",
    "# For forecasting, we extend the df_full timeline. We need the same feature logic.\n",
    "# We'll generate future rows by hour and fill exogenous variables with simple defaults.\n",
    "# In a real production system, you'd replace this with weather forecasts and holiday calendars.\n",
    "\n",
    "df_fc = df_full.copy().sort_values([\"dteday\",\"hr\"]).reset_index(drop=True)\n",
    "\n",
    "last_ts = df_fc[\"timestamp\"].max()\n",
    "future_ts = pd.date_range(last_ts + pd.Timedelta(hours=1), periods=FORECAST_HOURS, freq=\"H\")\n",
    "future = pd.DataFrame({\n",
    "    \"timestamp\": future_ts,\n",
    "    \"dteday\": future_ts.floor(\"D\"),\n",
    "    \"hr\": future_ts.hour\n",
    "})\n",
    "\n",
    "# Fill calendar fields using the last known values by day (simple, robust baseline)\n",
    "# season/yr/mnth copied from last available day with same month/day context is not possible;\n",
    "# so we copy from the last observed day (good enough for the assignment demo).\n",
    "last_day = df_fc[\"dteday\"].max()\n",
    "last_day_row = df_fc[df_fc[\"dteday\"] == last_day].iloc[-1]\n",
    "\n",
    "for col in [\"season\",\"yr\",\"mnth\",\"weathersit\",\"temp\",\"atemp\",\"hum\",\"windspeed\",\"holiday\",\"workingday\",\"weekday\"]:\n",
    "    if col in df_fc.columns:\n",
    "        future[col] = last_day_row[col]\n",
    "\n",
    "# Recompute weekday/workingday from date (prefer correctness)\n",
    "future[\"weekday\"] = future[\"dteday\"].dt.weekday\n",
    "# keep holiday default 0 unless you have a calendar feed\n",
    "future[\"holiday\"] = 0\n",
    "future[\"workingday\"] = ((future[\"weekday\"] < 5) & (future[\"holiday\"] == 0)).astype(int)\n",
    "\n",
    "# Append future rows with NaN targets (to be predicted)\n",
    "for col in TARGET_COLS:\n",
    "    future[col] = np.nan\n",
    "df_fc_ext = pd.concat([df_fc, future], ignore_index=True).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# Recursive prediction: fill cnt one step at a time\n",
    "def build_features_frame(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df_in.copy()\n",
    "    d[\"dteday\"] = pd.to_datetime(d[\"dteday\"])\n",
    "\n",
    "    d[\"lag_24\"] = d[\"cnt\"].shift(24)\n",
    "    d[\"lag_168\"] = d[\"cnt\"].shift(168)\n",
    "\n",
    "    d[\"season_name\"] = d[\"season\"].map(season_map)\n",
    "    season_dum = pd.get_dummies(d[\"season_name\"], prefix=\"season\")\n",
    "    d[\"hr_period\"] = d[\"hr\"].apply(hr_to_period)\n",
    "    hr_dum = pd.get_dummies(d[\"hr_period\"], prefix=\"hr\")\n",
    "\n",
    "    d[\"wd_morning\"] = ((d[\"workingday\"] == 1) & (d[\"hr_period\"] == \"morning\")).astype(int)\n",
    "    d[\"wd_evening\"] = ((d[\"workingday\"] == 1) & (d[\"hr_period\"] == \"evening\")).astype(int)\n",
    "\n",
    "    d[\"atemp_bucket\"] = pd.qcut(d[\"atemp\"], q=3, labels=[\"cold\",\"medium\",\"hot\"])\n",
    "    d[\"hum_bucket\"] = pd.qcut(d[\"hum\"], q=3, labels=[\"not_humid\",\"medium\",\"humid\"])\n",
    "    d[\"windspeed_bucket\"] = pd.qcut(d[\"windspeed\"], q=3, labels=[\"not_windy\",\"medium\",\"windy\"])\n",
    "\n",
    "    at_d = pd.get_dummies(d[\"atemp_bucket\"], prefix=\"atemp\")\n",
    "    hu_d = pd.get_dummies(d[\"hum_bucket\"], prefix=\"hum\")\n",
    "    wi_d = pd.get_dummies(d[\"windspeed_bucket\"], prefix=\"wind\")\n",
    "    ws_d = pd.get_dummies(d[\"weathersit\"].astype(\"Int64\"), prefix=\"weathersit\")\n",
    "\n",
    "    Xf = pd.concat(\n",
    "        [\n",
    "            d[NUM_FEATURES + BEHAVIOR_FEATURES],\n",
    "            season_dum, hr_dum, at_d, hu_d, wi_d, ws_d\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # align columns to training X\n",
    "    Xf = Xf.reindex(columns=X.columns, fill_value=0)\n",
    "    return Xf\n",
    "\n",
    "# Predict future rows iteratively\n",
    "df_fc_ext = df_fc_ext.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "future_idx = df_fc_ext[df_fc_ext[\"cnt\"].isna()].index\n",
    "\n",
    "for i in future_idx:\n",
    "    Xf = build_features_frame(df_fc_ext.iloc[:i+1])\n",
    "    x_last = Xf.iloc[[i]]\n",
    "    pred = float(gbr.predict(x_last)[0])\n",
    "\n",
    "    # production guards\n",
    "    pred = max(0.0, pred)\n",
    "    pred = round(pred)\n",
    "\n",
    "    df_fc_ext.loc[i, \"cnt\"] = pred\n",
    "\n",
    "# Extract forecast\n",
    "forecast = df_fc_ext.loc[future_idx, [\"timestamp\",\"dteday\",\"hr\",\"cnt\"]].copy()\n",
    "forecast[\"cnt\"] = forecast[\"cnt\"].astype(\"Int64\")\n",
    "\n",
    "print(\"Forecast sample:\")\n",
    "print(forecast.head(24).to_string(index=False))\n",
    "\n",
    "daily_forecast = forecast.groupby(\"dteday\")[\"cnt\"].sum().reset_index(name=\"cnt_daily_forecast\")\n",
    "print(\"\\nDaily forecast totals:\")\n",
    "print(daily_forecast.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
